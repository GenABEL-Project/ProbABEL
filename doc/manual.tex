\title{ProbABEL manual}
\author{Yurii Aulchenko, Maksim Struchalin \\
	Erasmus MC Rotterdam
}
\date{\today}

\documentclass[12pt]{article}
\usepackage{verbatim}
\usepackage{titleref}
\usepackage{makeidx}

\makeindex

\begin{document}
\maketitle
\tableofcontents

%\begin{abstract}
%This document describes ProbABEL package for analysis 
%of genome-wide imputed data \ldots
%\end{abstract}

\section{Motivation}

Many statistical and experimental techniques, such as imputations and 
high-throughput sequencing, generate the data, which are informative for 
genome-wide association analysis, and are probabilistic in the nature. 

When we work with directly genotyped markers using such techniques as 
SNP or microsatellite typing, we would normally know the genotype of 
a particular person at a particular locus with very high degree of 
confidence, and, in case of biallelic marker, can state whether 
genotype is $AA$, $AB$ or $BB$. 

On the contract, when dealing with imputed or 
high-throughput sequencing data, for many of genomic loci 
we are quite uncertainty about genotypic status of the person; 
instead of known genotypes  
we deal rather with probability distribution; that is based on 
observed information, we have estimates that true underlying 
genotype is either $AA$, $AB$ or $BB$; the degree of confidence 
about the real status is measured with 
probability distribution $\{P(AA), P(AB), P(BB)\}$.

Several techniques may be applied to analyse such data. The most 
simplistic approach would be to pick up the genotype with highest 
probability, i.e. $\max_g[P(g=AA), P(g=AB), P(g=BB)]$ and then 
analyse the data as if directly typed markers were used. The 
disadvantage of this approach is that it does not take into 
account the probability distribution -- i.e. the uncertainty 
about the true genotypic status. Such 
analysis is statistically wrong: the estimates of association 
parameters (regression coefficients, odds or hazard ratios, etc.) 
are biased, and the bias becomes more pronounced with greater 
probability distribution uncertainty (entropy). 

One of the solution which generates unbiased estimates 
of association parameters and takes 
probability distribution into account is achieved by 
performing association analysis by means of regression of the 
outcome of interest onto estimated genotypic probabilities. 

\texttt{ProbABEL} package was designed to perform such regression in fast, 
memory-efficient and consequently genome-wide feasible manner. 
Currently, \texttt{ProbABEL} implements linear, logistic regression, 
and Cox proportional hazards models. The corresponding analysis 
programs are called \texttt{palinear},  \texttt{palogist},  
and \texttt{pacoxph}.


\section{Input files}
\texttt{ProbABEL} takes three files as input: a file containing SNP 
information (e.g. MLINFO file of MACH), file with genome- or 
chromosome-wide predictor information (e.g. MLDOSE or MLPROB file of MACH), 
and a file containing phenotype of interest and covariates. 

Optionally, the map information can be supplied (e.g. "legend" 
files of HapMap). 

The dose/probbaility file may be supplied in filevector format 
in which case \texttt{ProbABEL} will operate much faster, and 
in low-RAM mode (approx. 128 Mb). See R libraries GenABEL and 
DatABEL on how to convert MACH and IMPUTE files to 
filevector format (functions: mach2databel, impute2databel).

\subsection{SNP information file}
\label{ssec:infoin}
In the simplest scenario, SNP information file is an MLINFO 
file generated by MACH. This must be a space or tab-delimited file 
containing SNP name, coding for allele 1 and 2 (e.g. A, T, G or C), 
frequency of allele 1, minor allele frequency and two quality 
metrics ("Quality" = average maximum posterior probability and 
"Rsq" -- proportion of variance decrease after imputations). 

Actually, 
for \texttt{ProbABEL}, it does not matter what is written in this file -- 
this information is just brought forward to the output. However, 
\textbf{it is critical} that the number of columns is seven and the number 
of lines in the file is equal to the number of SNPs in the 
corresponding DOSE file (plus one for the header line). 

The example of SNP information file content follows here (also 
to be found in \texttt{ProbABEL/example/test.mlinfo})

\verbatiminput{../example/test.mlinfo}

Note that header line is present in the file. The file describes 
five SNPs. 

\subsection{Genomic predictor file}
\label{ssec:dosein}

Again under simplest scenario this is a MLDOSE or MLPROB file generated by MACH.
Such file starts with two special columns plus, for each of the SNPs 
under consideration, a column containing the estimated allele 1 dose (MLDOSE).
In MLPROB file, two columns for each SNP correspond to posterior probability 
that person has two ($P_{A_1A_1}$) or one ($P_{A_1A_2}$) copies of allele 1. 
The first ''special'' column is made of the sequential id, 
followed by an arrow followed by study ID (the one specified in 
MACH input files). The second column contains method 
(e.g. ''MLDOSE'') keyword.

An example of the few first lines of an MLDOSE file for 
five SNPs described in SNP information file follows here (also 
to be found in \texttt{ProbABEL/example/test.mldose})

\verbatiminput{short_test.mldose}

\textbf{The order of SNPs in the SNP information file and DOSE-file
must be the same}. This should be the case if you just used MACH outputs.

Thus, by all means, the number of columns in the genomic predictor file 
must be the same as the number of lines in the SNP information file plus one. 

\subsection{Phenotypic file}
\label{ssec:phenoin}

Phenotypic data file contains phenotypic data, but also specifies the 
analysis model. There is a header line, specifying the variable names. 
The first column should contain personal study IDs. It is assumed 
that \textbf{both the total number and the order of these IDs is are 
exactly the same as in the genomic predictor (MLDOSE) file described in 
previous section}. This is not difficult to arrange using e.g. \texttt{R}; 
example is given in \texttt{ProbABEL/examples} directory. 

\textbf{Missing data should be coded with 'NA', 'N' or 'NaN' codes.} Any 
other coding will be converted to some number which will be used in 
analysis! E.g. coding missing as '-999.9' will result in analysis which 
will consider -999.9 as indeed true measurements of the trait/covariates.  

In case of linear or logistic regression (programs \texttt{palinear} and 
\texttt{palogist}, respectively), the second column specifies the trait 
under analysis, while the third, fourth, etc. 
provide information on covariates to be included into analysis. 
An example few lines of phenotypic information file designed for 
linear regression analysis follow here (also 
to be found in \texttt{ProbABEL/example/height.txt})

\verbatiminput{short_height.txt}

Note again that the order of IDs is the same between MLDOSE and phenotypic 
data file. The model specified by this file is $height \sim \mu + sex + age$, 
where $\mu$ is intercept.  

Clearly, you can for example include \texttt{sex x age} interaction terms by 
specifying another column having a product of sex and age here.

For logistic regression, it is assumed that in the second column cases are 
coded as ''1'' and controls as ''0''. An example few lines of phenotypic 
information file designed for logistic regression analysis follow here (also 
to be found in \texttt{ProbABEL/example/logist\_data.txt})

\verbatiminput{short_logist_data.txt}

You can see that in the first 10 people, there are three cases, as indicated 
by ''chd'' equal to one. The model specified by this file 
is $chd \sim \mu + sex + age + othercov$.  

In case of Cox proportional hazards model, the composition of the 
phenotypic input file is a bit different. In the second column and 
third column, you need to specify the outcome in terms of follow-up 
time (column two) and event (column three, ''1'' if event occurred 
and zero if censoring). Columns from four inclusive specify covariates 
to be included into analysis. An example few lines of phenotypic 
information file designed for Cox proportional hazards model 
analysis follow here \\(also to be found in
\texttt{ProbABEL/example/coxph\_data.txt})

\verbatiminput{short_coxph_data.txt}

You can see that for first 10 people, event happens for three of 
them, while for the other seven there is no event during follow-up 
time, as indicated 
by ''chd'' column. Follow-up time is specified in the preceding 
column. The covariates included into the model are age (presumably 
at baseline), sex and ''othercov''; thus the model, in terms of 
\texttt{R/survival} is \\ $Surv(fuptime\_chd, chd) \sim sex + age + othercov$.  

\subsection{Optional map file}
If you would like that map information (e.g. base pair position) to 
be included in your outputs, you can supply a map file. These follow 
HapMap "legend" file format. For example, for the five SNPs we considered 
the map-file may look like

\verbatiminput{../example/test.map}

The order of the SNPs in the map file should follow that in the SNP information 
file. Only information from the second column -- the SNP location -- is 
actually used to generate the output.

\section{Running analysis}

To run linear regression, you should use program called \texttt{palinear};
for logistic analysis use \texttt{palogist}, and for Cox proportional 
hazards model use \texttt{pacoxph} (to be found in 
\texttt{ProbABEL/bin/} directory after you have compiled the program).

There are in total 11 command line options you can specify to \texttt{ProbABEL} 
analysis functions \texttt{linear} or \texttt{logistic}. If you run 
either program without any argument, you 
will get a short explanation to command line options:

\begin{verbatim}
user@server~$ palogist

Usage: ../bin/palogist options

Options:
		--pheno       : phenotype file name
		--info        : information (e.g. MLINFO) file name
		--dose        : predictor (e.g. MLDOSE/MLPROB) file name
		--map         : [optional] map file name
		--nids        : [optional] number of people to analyse
		--chrom       : [optional] chromosome (to be passed to output)
		--out         : [optional] output file name (default is regression.out.txt)
		--skipd       : [optional] how many columns to skip in predictor
								    (dose/prob) file (default 2)
		--ntraits     : [optional] how many traits are analysed (default 1)
		--ngpreds     : [optional] how many predictor columns per marker
								    (default 1 = MLDOSE; else use 2 for MLPROB)
		--separat     : [optional] character to separate fields (default is space)
		--score       : use score test
		--no-head     : do not report header line
		--allcov      : report estimates for all covariates (large outputs!)
		--interaction : which covariate to use for interaction with SNP
									  (default is no ineraction, 0)
		--interaction_only: like previos but without covariate acting in
                    interaction with SNP
									  (default is no ineraction, 0)
		--mmscore     : score test for association between a trait and genetic
                    polymorphism, in samples of related individuals	
		--robust      : report robust (aka sandwich, aka Hubert-White) standard 
                    errors
		--help        : print help

\end{verbatim}


\subsection{Basic analysis options}
However, for 
a simple run you can use only three, which specify the necessary files 
needed to run regression analysis.

These options are 
\texttt{--dose} (or \texttt{-d}), 
specifying genomic predictor / MLDOSE file described in sub-section \ref{ssec:dosein};
\texttt{--pheno} (or \texttt{-p}), 
specifying the phenotypic data file described in sub-section \ref{ssec:phenoin}; and
\texttt{--info} (or \texttt{-i}), 
specifying the SNP information file described in sub-section \ref{ssec:infoin}.

If you change to the \texttt{ProbABEL/example} directory you can run analysis of 
height by 

\begin{verbatim}
user@server~/ProbABEL/example/$ ../bin/palinear -p height.txt 
-d test.mldose -i test.mlinfo
\end{verbatim}

Output from analysis will be directed to "regression.out.csv" file.

You can run analysis of binary trait "chd" with 

\begin{verbatim}
user@server~/ProbABEL/example/$ ../bin/palogist -p logist_data.txt 
                                 -d test.mldose -i test.mlinfo
\end{verbatim}

To run a Cox proportional hazards model, try 

\begin{verbatim}
user@server~/ProbABEL/example/$ ../bin/pacoxph -p coxph_data.txt 
                                 -d test.mldose -i test.mlinfo
\end{verbatim}

Please have a look at the shell script files \texttt{example\_qt.sh}, 
\texttt{example\_bt.sh} and \texttt{example\_all.sh} to have 
a better overview of analysis options.

To run analysis with MLPROB files, you need specify the MLPROB file 
with -d option and also specify that there are two genetic predictors 
per SNP, e.g. you can run linear model with

\begin{verbatim}
user@server~/ProbABEL/example/$ ../bin/palinear -p height.txt 
                                 -d test.mlprob -i test.mlinfo --ngpreds=2
\end{verbatim}

\subsection{Advanced analysis options}

Option \texttt{--interaction} allows you to include interaction between SNP 
and any covariate. If e.g. your model is \texttt{trait ~ sex + age + SNP}, 
running the program with option \texttt{--interaction=2} will model 
\texttt{trait ~ sex + age + SNP + age*SNP}.

Option \texttt{--robust} allows you to compute so-called ''robust'' (aka
''sandwich'', aka Hubert-White) standard errors (see section ``Methodology''
for details).

With option \texttt{--mmscore} score test for association between a trait and genetic
polymorphism  in samples of related individuals is performed. File with inverse of variance-covarince matrix goes as input parameter
with that key. Like \texttt{--mmscore <filename>}. The file has to contain the first column with id names exactly like in phenotype file, BUT OMITTING people with no measured phenotype. The rest is a matrix.
Phenotype file in case of using key --mscore may contain any amount of covariates (opposed to previous versions). The first is id names, the second - trait. 
Otheres are covariates.

An example how polygenic object estimated by GenABEL can be used with ProbABEL
is provided here: example/mmscore.R

Though technically \texttt{--mmscore} allows for inclusion of multiple 
covariates, these should be kept to minimum as this is score test. We suggest 
that any covariates explaining essential proportion of variance should be 
fit as part of the \texttt{GenABEL}'s \texttt{polygenic} procedure.

Option \texttt{--interaction\_only} is like \texttt{--interaction} but does not
include in the model the main effect of the covariate, which is acting in
interaction with SNP. This option is useful when running \texttt{--mmscore}, 
in whch case the main effect should normally estimated in the polygenic 
model and only the interaction term in the \texttt{ProbABEL} analysis.

\subsection{Running multiple analyses at once: probabel.pl}

Perl script \texttt{bin/probabel.pl\_example} represents a handy wraper for ProbABEL functions. 
To start using it you have to change config file \\
\texttt{bin/probabel\_config.cfg\_example}.
Configuration file consists of 5 columns. Each column except of the first is pattern for 
files produced by \texttt{MACH} (imputation software). 
Column named "cohort" is name of population ("ERGO" in this example), column "mlinfo\_path" -- 
full path to mlinfo files and pattern of name where chromosome number has been 
replaced by "\_.\_chr\_.\_". Columns "mldose\_path", "mlprobe\_path" and "legend\_path" 
are paths and patterns for "mldose", "mlprob" and "legend" files.
Probably you also have to change variable \texttt{\$config} in sript to point full path to 
configuration file and variable \texttt{@anprog } to point full path to ProbABEL scripts.


\section{Output file format}

Let us consider what comes out of the linear regression analysis 
described in the above section. After you have run the analysis, in 
the output file you will find something like

\begin{small}
\verbatiminput{short_height.out.csv}
\end{small}

Here, I show only three first lines of output. Note that lines 
starting with ''...'' are actually the ones continuing the 
previous line -- I just have wrapped this output so we can see 
these long lines. 

The header provides short description of what can be found in the 
specific column. The first column provides the SNP name and 
next six are descriptives which were brought directly from the 
SNP information file. Thus these describe allele frequencies and 
quality in your total imputations, not necessarily in the data under 
analysis. 

On the contrast, starting with the next column, named ''n'', 
the output concerns the data analysed. Column 8 (''n'') tells the 
number of subjects for whom complete phenotypic information was available. 
At this point, unless you have complete measurements on all 
subjects, you should feel warned if the number here is exactly the 
number of people in the file -- this probably indicates you did not code 
missing values according to \texttt{ProbABEL} format ('NA', 'NaN', or 'N').

The next column nine (''Mean\_predictor\_allele'') gives you estimated 
frequency of the predictor allele in subjects with complete phenotypic data. 

If ''--chrom'' option was used, in the next column you will find the 
value specified by this option. If ''--map'' option was used, in next 
column you will find map location brought from the map-file. Next 
columns provide coefficients of regression  of the phenotype 
onto genotype, corresponding standard errors, and the $\chi^2$ 
of the Likelihood Ratio Test for deviation from zero.

\section{Preparing input files}

In the \texttt{ProbABEL/bin} directory you can find \texttt{perepare\_data.R}
file -- an R script which arranges phenotypic data in right format. 
Please read this script for details.

\section{Memory use and performance}

Maximum likelihood regression is implemented in \texttt{ProbABEL}. With 6,000 
people and 2.5 millions SNPs, genome-wide scan is completed in less 
that an hour for linear model with 1-2 covariates and overnight 
for logistic regression or Cox proportional hazards model.

Memory is an issue with \texttt{ProbABEL} -- large chromosomes, 
such as chromosome one consumed up to 5 Gb RAM with 6,000 people. 

\section{Methodology}

\subsection{Analysis of population-based data}

\subsubsection{Linear regression assuming normal distribution}

Standard linear regression theory is used to estimate coefficients of
regression and their standard errors. We assume linear model with
expectation
\begin{equation}
E[\mathbf{Y}] = \mathbf{X} \mathbf{\beta}
\label{expectation}
\end{equation}
and variance-covariance matrix 
$$
\mathbf{V} = \sigma^2 \mathbf{I}
$$
where $\mathbf{Y}$ is the vector of phenotypes of interest, 
$\mathbf{X}$ is design matrix, $\mathbf{\beta}$ is the vector of regression 
parameters, $\sigma^2$ is variance and $\mathbf{I}$ is identity matrix. 

The maximum likelihood estimates (MLEs) for the regression parameters 
is given by
\begin{equation}
\hat{\beta} = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{Y}
\end{equation}
and MLE of the residual variance is
\begin{equation}
\hat{\sigma}^2 = \frac{(\mathbf{Y} - \mathbf{X}\hat{\beta})^T (\mathbf{Y} - \mathbf{X}\hat{\beta})}
			{N-r_X}
\end{equation}
where $N$ is the number of observations and $r_X$ is rank of $\mathbf{X}$ 
(number of columns of the design matrix). 

The variance-covariance matrix for the parameter estimates under 
alternative hypothesis can be 
computed as 
\begin{equation}
\mathbf{var}_{\hat{\beta}} = \hat{\sigma}^2 (\mathbf{X}^T\mathbf{X})^{-1}
\end{equation}

For the $j$-the element $\hat{\beta}(j)$ of the vector of estimates the standard 
error under alternative hypothesis is given by the square root of the 
corresponding diagonal element of the above matrix, $\mathbf{var}_{\hat{\beta}}(jj)$,
and the Wald test can be computed with
$$
T^2(j) = \frac{ \hat{\beta}(j)^2 }{ \mathbf{var}_{\hat{\beta}}(jj) }
$$
which asymptotically follows the $\chi^2$ distribution with one degree of 
freedom under the null hypothesis. 

When testing significance for more than one parameter simultaneously, 
several alternatives are available. Let us first partition the vector of 
parameters into two components, $\beta = (\beta_g,\beta_x)$, and our 
interest is testing the parameters contained in $\beta_g$ (SNP effects), 
while $\beta_x$ (e.g. effects of sex, age, etc.) are considered nuisance 
parameters. Let us define the vector of the parameters of interest 
which are fixed to certain values under the null hypothesis as $\beta_{g,0}$. 

Firstly, the likelihood ratio test can be obtained with
$$
LRT = 2 \cdot (logLik(\hat{\beta}_g,\hat{\beta}_x) - logLik(\beta_{g,0},\hat{\beta}_x))
$$
which under the null hypothesis is asymptotically distributed as $\chi^2$ with 
number of degrees of freedom equal to the number of parameters specified 
by $\beta_g$. Assuming the normal distribution, the log-likelihood of a 
model specified by the vector of parameters $\beta$ and residual variance 
$\sigma^2$ can be computed as
$$
logLik(\beta,\sigma^2) = 
-\frac{1}{2} ( N \cdot log_e \sigma^2 +
(\mathbf{Y} - \beta \mathbf{X})^T (\mathbf{I}/\sigma^2) (\mathbf{Y} - \beta \mathbf{X}) )
$$

Secondly, the Wald test can be used; for that the inverse variance-covariance
matrix of $\hat{\beta}_g$ should be computed as 
$$
\mathbf{var}_{\hat{\beta}_g}^{-1} = \mathbf{var}_{\hat{\beta}}^{-1}(g,g) - 
\mathbf{var}_{\hat{\beta}}^{-1}(g,x) (\mathbf{var}_{\hat{\beta}}^{-1}(x,x))^{-1} \mathbf{var}_{\hat{\beta}}^{-1}(x,g)
$$
where $\mathbf{var}_{\hat{\beta}}^{-1}(a,b)$ correspond to sub-matrices of the inverse of the 
variance-covariance matrix of $\hat{\beta}$, involving either only parameters of interest 
$(g,g)$, nuisance parameters $(x,x)$ or combination of these $(x,g)$, $(g,x)$.

The Wald test statistics is then computed as 
$$
W^2 = (\hat{\beta}_g - \beta_{g,0})^T \mathbf{var}_{\hat{\beta}_g}^{-1} (\hat{\beta}_g - \beta_{g,0})
$$
which asymptotically follows the $\chi^2$ distribution with the number of degrees 
of freedom equal to the number of parameters specified by $\beta_g$. The Wald test generally 
is computationally easier than the LRT, because it avoids estimation of the model 
specified by the parameter's vector $(\beta_{g,0},\hat{\beta}_x)$.

Lastly, similar to the Wald test, the score test can be performed by use 
of $\mathbf{var}_{(\beta_{g,0},\hat{\beta}_x)}$ instead of $\mathbf{var}_{\hat{\beta}}$.

%% Comparative advantages of these testing approaches in the context of GWAS will be discussed in
%% ''\titleref{implementation}'' section (\ref{implementation}).

\subsubsection{Logistic regression}

For logistic regression, the procedure to obtain 
parameters estimates, their variance-covariance matrix, and tests are 
similar to these outlined above with several modifications. 

The expectation of the binary trait is defined as expected 
probability of the event as defined by the logistic 
function 
$$
E[\mathbf{Y}] = \pi = \frac{ 1 }{ 1 + e^{-(\mathbf{X}\beta)} }
$$

The estimates of the parameters are obtained not in one 
step, as is the case of the linear model, but using iterative 
procedure (iteratively re-weighted least squares). This 
procedure is not described here for the sake of brevity. 

The log-likelihood of the data is computed using 
binomial probability formula:
$$
logLik(\beta) = \mathbf{Y}^T log_e \pi + (\mathbf{1} - \mathbf{Y})^T log_e (\mathbf{1}-\pi)
$$
where $log_e \pi$ is a vector obtained by taking the natural logarithm of every 
value contained in the vector $\pi$. 

\subsubsection{Robust variance-covariance matrix of parameter estimates}

For linear model, these are computed using formula
$$
\mathbf{var}_r = (\mathbf{X}^T\mathbf{X})^{-1} (\mathbf{X}^T\mathbf{R}\mathbf{X})
(\mathbf{X}^T\mathbf{X})^{-1}
$$
where $\mathbf{R}$ is a diagonal matrix containing squares of residuals
of $\mathbf{Y}$. The 
same formula may be used for ``standard'' analysis, in which case
the elements of the $\mathbf{R}$ matrix are constant, namely mean 
residual sum of squares (the estimate of $\sigma^2$).

Similar to that, the robust matrix is computed for logistic regression with
$$
\mathbf{var}_r = (\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1} (\mathbf{X}^T\mathbf{R}\mathbf{X})
(\mathbf{X}^T \mathbf{W} \mathbf{X})^{-1}
$$
where $\mathbf{1}$ is the vector of ones and $\mathbf{W}$ is the diagonal matrix 
of ''weights'' used in logistic regression. 


\subsubsection{Cox proportional hazards model}

The implementation of the Cox proportional hazard model used 
\index{Cox proportional hazards model}
\index{proportional hazards model}
\index{regression!Cox proportional hazards}
in \texttt{ProbABEL} is entirely based on the code of \texttt{R} 
library \texttt{survival} developed by Thomas Lumley 
(function \texttt{coxfit2}), and is therefore not described here. 

Many thanks to Thomas for making his code available under GNU GPL!

\subsection{Analysis of pedigree data}

The framework for analysis of pedigree data follows the two-step 
logic developed in the works of Aulchenko et al. (2007) and Chen and Abecasis
(2007). General analysis model is a linear mixed model which defines the
expectation of the trait as 
$$
E[\mathbf{Y}] = \mathbf{X} \mathbf{\beta}
$$
identical to that defined for linear model (\ref{expectation}).
To account for correlations between the phenotypes of
relatives which may be induced by family relations the variance-covariance 
matrix is defined to be proportional to the linear combination of the
identity matrix $\mathbf{I}$ and the relationship matrix $\mathbf{\Phi}$: 
$$
\mathbf{V}_{\sigma^2,h^2} = \sigma^2 (2 h^2 \mathbf{\Phi} + (1-h^2) \mathbf{I})
$$
where $h^2$ is the heritability of the trait. 
The relationship matrix $\mathbf{\Phi}$ is twice the matrix containing 
the coefficients of kinship between all pairs of individuals under consideration; 
its estimation is discussed in a separate section ''\titleref{kinship}'' (\ref{kinship}).

Estimation of thus defined model is possible by numerical maximization of the
likelihood function, however, the estimation of this model 
for large pedigrees is laborious, and is not computationally feasible for
hundreds of thousands to millions of SNPs to be tested in the context of GWAS,
as we have demonstrated previously (Aulchenko et al., 2007).

\subsubsection{Two-step score test for association}

A two-step score test approach is therefore used to decrease the computational
burden. Let us first re-define the expectation of the trait by splitting the 
design matrix in two parts, the ''base'' part $\mathbf{X}_x$, which includes all 
terms not changing across all SNP models fit in GWAS (e.g. effects of sex, age, etc.), 
and the part including SNP information, $\mathbf{X_g}$:
$$
E[\mathbf{Y}] = \mathbf{X}_x \mathbf{\beta}_x + \mathbf{X}_g \mathbf{\beta}_g
$$

Note that the latter design matrix may include not only the main SNP effect, but 
e.g. SNP by environment interaction terms.

At the first step, linear mixed model not including SNP effects
$$
E[\mathbf{Y}] = \mathbf{X}_x \mathbf{\beta}_x
$$
is fitted. The maximum likelihood estimates (MLEs) of the model parameters (regression coefficients for 
the fixed effects $\hat{\mathbf{\beta}}_x$, the
residual variance $\hat{\sigma}^2_x$ and the heritability $\hat{h}^2_x$) can 
be obtained by numerical maximization of the likelihood function
$$
logLik(\beta_x,h^2,\sigma^2) = -\frac{1}{2} ( log_e|\mathbf{V}_{\sigma^2,h^2}|+
(\mathbf{Y} - \beta_x \mathbf{X}_x)^T \mathbf{V}_{\sigma^2,h^2}^{-1} (\mathbf{Y} - \beta_x \mathbf{X}_x) )
$$
where $\mathbf{V}_{\sigma^2,h^2}^{-1}$ is the inverse and 
$|\mathbf{V}_{\sigma^2,h^2}|$ is the determinant of the variance-covariance matrix. 

At the second step, the unbiased estimates of the fixed effects of the terms 
involving SNP are obtained with 
$$
\hat{\beta}_g = (\mathbf{X}^T_g \mathbf{V}^{-1}_{\hat{\sigma}^2,\hat{h}^2} \mathbf{X}_g)^{-1} 
		\mathbf{X}^T_g \mathbf{V}^{-1}_{\hat{\sigma}^2,\hat{h}^2} \mathbf{R}_{\hat{\beta}_x}
$$
where $\mathbf{V}^{-1}_{\hat{\sigma}^2,\hat{h}^2}$ is the variance-covariance matrix at the point 
of the MLE estimates of $\hat{h}^2_x$ and $\hat{\sigma}^2_x$ and 
$\mathbf{R}_{\hat{\beta}_x} = \mathbf{Y} - \hat{\beta}_x \mathbf{X}_x$ is the 
vector of residuals obtained from the base regression model. Under the null 
model, the inverse variance-covariance matrix of the parameter's estimates is defined 
as
$$
\mathbf{var}_{\hat{\beta}_g} = \hat{\sigma}^2_x (\mathbf{X}^T_g \mathbf{V}^{-1}_{\hat{\sigma}^2,\hat{h}^2} \mathbf{X}_g)^{-1}
$$

Thus the score test for joint significance of the terms involving SNP can be obtained with 
$$
T^2 = (\hat{\beta}_g - \beta_{g,0})^T \mathbf{var}_{\hat{\beta}_g}^{-1} (\hat{\beta}_g - \beta_{g,0})
$$
where $\beta_{g,0}$ are the values of parameters fixed under the null model. 
This test statistics under the null hypothesis asymptotically follows the 
$\chi^2$ distribution with the number of degrees 
of freedom equal to the number of parameters tested. 
The significance of an individual $j$-the elements of the vector $\hat{\beta}_g$ can be tested with
$$
T^2_j = \hat{\beta}_{g}^2(j) \mathbf{var}_{\hat{\beta}_g}^{-1}(jj)
$$
where $\hat{\beta}_{g}^2(j)$ is square of the $j$-th element of the vector of estimates $\hat{\beta}_{g}$, 
and $\mathbf{var}_{\hat{\beta}_g}^{-1}(jj)$ corresponds to the $j$-th diagonal element of 
$\mathbf{var}_{\hat{\beta}_g}^{-1}$.
The latter statistics asymptotically follows $\chi^2_1$.

\subsubsection{Estimation of the kinship matrix}
\label{kinship}

The relationship matrix $\mathbf{\Phi}$ used in estimation of the 
linear mixed model for pedigree data is twice the matrix containing 
the coefficients of kinship between all pairs of individuals under consideration. 
This coefficient is defined as the probability that two gametes randomly sampled 
from each member of the pair are identical-by-descent (IBD), that is they are copies 
of exactly the same ancestral allele. The expectation of kinship 
can be estimated from pedigree data using standard methods, for example the 
kinship for two outbred sibs is $1/4$, for grandchild-grandparent is $1/8$, etc.
For an outbred person, the kinship coefficient is $1/2$ -- that is two gametes 
sampled from this person at random are IBD only if the same gamete is 
sampled. However, if the person is inbred, there is a chance that a maternal 
and paternal chromosomes are also IBD. The probability of this is characterized 
by kinship between individual's parents, which is defined as the individual's
inbreeding coefficient, $F$. In this case, the kinship coefficient for the 
individual is $F + 1/2$. Similar logic applies to computation of the kinship 
coefficient for other types of pairs in inbred pedigrees. 

The kinship matrix can be computed using the pedigree data using standard methods.
However, in many cases, pedigree information may be absent, incomplete, or not 
reliable. Moreover, the estimates obtained using pedigree data reflect the 
expectation of the kinship, while the true realization of kinship may vary 
around this expectation. In presence of genomic data it may therefore be 
desirable to estimate the kinship coefficient from these, and not from pedigree. 
It can be demonstrated that unbiased and positive semi-definite estimator 
of the kinship matrix can be obtained (Astle and Balding, 2010; Amin et al., 2007)
by computing the kinship coefficients between individuals $i$ and $j$ with
$$
\hat{K}_{ij} = \frac{1}{L} \sum_{l=1}^L \frac{ (g_{l,i} - p_l) (g_{l,j} - p_l) }{ p_l (1-p_l) }
$$
where $L$ is the number of loci, $p_l$ is the allelic frequency at $l$-th locus
and $g_{l,j}$ is the genotype of $j$-th person at the $l$-th locus, coded 
as $0$, $1/2$, and $1$, corresponding to the homozygous, heterozygous, and 
other type of homozygous genotype. The frequency is computed for the allele 
which, when homozygous, corresponds to the genotype coded as ''1'.


\section{How to cite}

As for May 2008, we have not yet published 
\texttt{ProbABEL} paper. If you used \texttt{ProbABEL} for 
your analysis please give a link to the \texttt{ABEL} home 
page 

\begin{quote}
http://mga.bionet.nsc.ru/~yurii/ABEL/
\end{quote}

and cite \texttt{GenABEL} paper to give us some credit: 

\begin{quote}
Aulchenko YS, Ripke S, Isaacs A, van Duijn CM.
GenABEL: an R library for genome-wide association analysis.
Bioinformatics. 2007 23(10):1294-6. 
\end{quote}

Proper reference may look like 

\begin{quote}
For analysis of imputed data, we used \texttt{ProbABEL} package 
from the ABEL 
set of programs (Aulchenko et al., 2007). 
\end{quote}

If you have used Cox proportional hazard model, please mention 
\texttt{R} package \texttt{survival} by Thomas Lumley. Additionally 
to the above citation, please tell that 

\begin{quote}
Cox proportional hazards model implemented in \texttt{ProbABEL} 
makes use of the source code of \texttt{R} package ''\texttt{survival}'' 
as implemented by T. Lumley. 
\end{quote}


%\subsection{Generalized linear models}

%Let us define linear predictor $\eta$ as 
%$$
%\eta = \mathbf{X} \beta
%$$
%For regular linear regression model described in previous section 
%this linear predictor models the expectation of the trait of interest, 
%while 

%\begin{table}
%\begin{center}
%\caption{Link .. functions}
%\begin{tabular}{lccc}
%\hline
%Distribution & $\mu$ & $\tau^2$ & $v(\mu)$ \\
%\hline
%Normal	 &            $\eta$ & $\sigma^2$ & \\
%Binomial & $1/(1+e^{-\eta})$ & & \\
%Poisson  &        $e^{\eta}$ & & \\
%\hline
%\end{tabular}
%\end{center}
%\end{table}

%Iteratively re-weighted least squares are used to obtain parameters' 
%estimates. 

\printindex

\end{document}
